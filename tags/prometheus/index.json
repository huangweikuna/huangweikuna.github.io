[{"content":"大致流程 下面示意图可以帮助你调试 Kubernetes Deployment。它帮助了我回忆在操作集群时碰到的问题。图片较大，可以存到本地查看。\n我的检查操作一般为下面这几步，图中也有体现。\n kubectl describe ... 查看event是否有提示信息 evnet 没有信息查看两个探针是否正常 kubectl logs -f --tail=100 查看日志是否正常 一些奇怪的疏忽，镜像是否正常(比如需要开启tty)，漏CMD命令 Google  常见Pod错误 Pod 可能会出现启动和运行时错误。\n启动错误 ImagePullBackoff\nImageInspectError\nErrImagePull\nErrImageNeverPull\nRegistryUnavailable\nInvalidImageName 运行时错误 CrashLoopBackOff\nRunContainerError\nKillContainerError\nVerifyNonRootError\nRunInitContainerError\nCreatePodSandboxError\nConfigPodSandboxError\nKillPodSandboxError\nSetupNetworkError\nTeardownNetworkError\n有些错误比其他错误更常见。以下是最常见的错误列表以及如何修复它们的方法。\nImagePullBackOff 当 Kubernetes 无法获取到 Pod 中某个容器的镜像时，将出现此错误。可能的原因：\n镜像名称无效-例如，你拼错了名称，或者镜像不存在\n 你为镜像指定了不存在的标签 你尝试检索的镜像属于一个私有 registry，而 Kubernetes 没有凭据可以访问它 前两种情况可以通过修改镜像名称和标记来解决。针对第三种情况，你应该将私有 registry 的访问凭证通过 Secret 添加到 Kubernetes 中并在 Pod 中引用它。 镜像要翻墙，下载超时了  CrashLoopBackOff 如果容器无法启动，则 Kubernetes 将显示错误状态为：CrashLoopBackOff。\n通常，在以下情况下容器无法启动：\n 应用程序中存在错误，导致无法启动 你未正确配置容器 Liveness 探针失败太多次  你应该尝试从该容器中检索日志以调查其失败的原因。如果由于容器重新启动太快而看不到日志，则可以使用以下命令：\n1  kubectl logs \u0026lt;pod-name\u0026gt; --previous   这个命令可以打印前一个容器的错误日志信息。\nRunContainerError 当容器无法启动时，会出现此错误。甚至在容器内的应用程序启动之前。该问题通常是由于配置错误，例如：\n 挂载不存在的卷，例如 ConfigMap 或 Secrets 将只读卷挂载为可读写\n你应该使用kubectl describe pod命令收集和分析错误  Pending 状态的 Pod 和 未就绪状态的 Pod\n当创建 Pod 时，该 Pod 处于 Pending 状态。为什么？假设你的调度程序组件运行良好，可能的原因如下：\n 集群没有足够的资源（例如CPU和内存）来运行 Pod 当前的命名空间具有 ResourceQuota 对象，创建 Pod 将使命名空间超过配额 该 Pod 绑定到一个处于 pending 状态的 PersistentVolumeClaim  如果 Pod 正在运行但未就绪(not ready)，则表示 readiness 就绪探针失败。当“就绪”探针失败时，Pod 未连接到服务，并且没有流量转发到该实例。\n对于因 ResourceQuotas 而导致的错误，可以使用以下方法检查集群的日志：\n1  kubectl get events --sort-by=.metadata.creationTimestamp   Service 故障排查 如果你的 Pod 正在运行并处于就绪状态，但仍无法收到应用程序的响应，则应检查服务的配置是否正确。\nService 旨在根据标签将流量路由到对应的 Pod。因此，你应该检查的第一件事是服务关联了多少个 Pod。你可以通过检查服务中的端点(endpoint)来做到这一点：\n1  kubectl describe service \u0026lt;service-name\u0026gt; | grep Endpoints   端点是一个集合，并且在服务以 Pod 为目标时，应该至少有一个端点。如果“端点”部分为空，则有两种解释：\n 你没有运行带有正确标签的 Pod（应检查自己是否在正确的命名空间中） Service 的 selector 标签上有错字  如果你看到端点列表，但仍然无法访问你的应用程序，则 targetPort 很有可能不匹配导致的。\nIngress的故障排查 如果你已经排查到这一步了的话，则：\nPod 正在运行并准备就绪，Serivce 会将流量分配到 Pod，但是你仍然看不到应用程序的响应。这意味着最有可能是 Ingress 配置错误。由于正在使用的 Ingress 控制器是集群中的第三方组件，因此有不同的调试技术，具体取决于 Ingress 控制器的类型。\n但是在深入研究 Ingress 专用工具之前，你可以用一些简单的方法进行检查。\nIngress 使用 serviceName 和 servicePort 连接到 Service。你应该检查这些配置是否正确。你可以通过下面命令检查 Ingress 配置是否正确：\n1  kubectl describe ingress \u0026lt;ingress-name\u0026gt;   如果 backend 一列为空，则配置中必然有一个错误。如果你可以在“backend”列中看到端点，但是仍然无法访问该应用程序，则可能是以下问题：\n 你如何将 Ingress 暴露于公网的 你如何将集群暴露于公网的 你还可以通过直接连接到 Ingress Pod 来将基础问题与 Ingress 隔离开。首先，获取你的 Ingress 控制器 Pod（可以位于其他名称空间中）：  1  kubectl port-forward nginx-ingress-.. 3000:80 --namespace kube-system   此时，每次你访问计算机上的端口 3000 时，请求都会转发到 Pod 上的端口 80。现在可以用吗？\n如果可行，则问题出在基础架构中，你应该调查流量如何路由到你的集群。\n如果不起作用，则问题出在 Ingress 控制器中，你应该调试 Ingress。\n如果仍然无法使 Ingress 控制器正常工作，则应开始对其进行调试。目前有许多不同版本的 Ingress 控制器。热门选项包括 Nginx，HAProxy，Traefik 等，你应该查阅 Ingress 控制器的文档以查找故障排除指南。\n由于 Ingress Nginx 是最受欢迎的 Ingress 控制器，因此接下来我们将介绍一些有关调试 ingress-nginx 的技巧。\n调试 Ingress Nginx，Ingress-nginx 项目有一个 Kubectl 的官方插件。你可以使用 kubectl ingress-nginx ：\n 检查日志，后端，证书等。 连接到 ingress 检查当前配置  你应该尝试的三个命令是：\n kubectl ingress-nginx lint，它会检查 nginx.conf 配置 kubectl ingress-nginx backend，它会检查后端（类似于kubectl describe ingress） kubectl ingress-nginx logs，查看日志  参考 Kubernetes Deployment 故障排查常见方法[译]\n","description":"Kubernetes Deployment 常见故障排查方法.","id":0,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Deployment 常见故障排查方法","uri":"/posts/kubernetes%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95/"},{"content":"Prometheus介绍 首先看到Prometheus官方架构图：\nexporter和pushgatway 数据通过exporter和pushgatway收集存储起来，再由Prometheus服务进行拉取操作。这样的设计把数据压力分摊到了各个收集节点，不会导致收集节点过多而Prometheus承受不住数据冲击而宕机。\n对于收集的方式通过exporter或pushgatway暴露http/https端口地址进行采集，也就是除了官方提供的exporter之外，我们可以很好的进行定制exporter，只需满足Prometheus的数据格式，在暴露到地址即可。具体细节可以查看 开发自己的exporter。pushgatway 用于不方便去提供一个采集接口时，Prometheus提供的官方接口，可以直接把数据发到pushgatway中，这样也可以收集到发送到数据。但是pushgatway不保证数据的生命周期在你的管控之中，因为pushgatway的控制权在Prometheus，如果pushgatway宕机了，你发的数据就会丢失。所以官方推荐短期任务可以借助pushgatway，而不自己开发exporter。\nPrometheus Server 在使用Prometheus监控时，需要有一个念头：Prometheus监控指标，而不能详细到具体数据。而指标不是百分百精确，它有大概的趋势，来帮助我们发现和预判问题。如果需要详细的问题调查体系，则需要日志收集框架和链路追踪框架支撑。所以查看问题的顺序是这样子的：指标监控-\u0026gt;日志查询-\u0026gt;链路追踪查询，获得的信息逐渐增多，但是所要做的事也增多了。\n了解了上面的信息，还需要注意的是：Prometheus认为只有最近的监控数据才有查询的需要，所有Prometheus本地存储的设计初衷只是保存短期（如一个月）的数据，不会针对大量的历史数据进行存储。也就是说，数据保留可能不会太久（长久的数据价值可能远超保留它的代价）。如果需要存储，则需要其他的扩展方案。在图中可以看到，对于存储Prometheus使用自带的TSDB进行存储，对于采集节点服务发现和Kubernetes集成，这也是很关键的一点。\n数据展示、报警 Prometheus收集数据存储到本地，这些数据可以配置成rule、alert两种聚合数据，rule会被Grafana收集进行展示、alert会被Alertmanager进行收集报警。这两个组件的灵活度都特别的高，但是不是本节重点，继续往后看。\n使用 在之前开发流程是这样的，下载Prometheus、Exporter、Grafana、Alertmanager。之后在Prometheus中配置对应的exporter信息，rule、alert。当然这些都可以通过Docker或者Kubernetes的配置文件一件部署，在kubernetes下则可能是这样子的：\n可以看到Prometheus连接着其他的组件。当然，连接会通过Service组件连接，防止exporter或组件宕机、升级在集群中重新调度时的ip丢失。但是如果需要添加组件，如新的采集节点，则需要手动修改配置文件。这样维护起来在大集群里可能成本比较高。于是之后就出现了Prometheus-Operator。\nPrometheus-Operator是在Kubernetes中的一组CRD，所以它所有的操作都是依赖于Kubernetes的。通过Kubernetes平台优化了上面的问题和添加功能，先看看架构图：\n可以看到在Prometheus和组件的连接中又抽象了一层ServiceMonitor，ServiceMonitor通过label进行选择组件的连接，和Kubernetes中的一样，我们通过操作类似label选择器的机制去管理组件。除了和Prometheus的连接管理之外，Operator还管理着Grafana和Alertmanager，它们各自的配置文件都可以动态进行更新，此外还抽象出了rule组件，这样rule、alert都可以进行动态更新和管理。\n上面所有的管理工作都是由Operator组件完成，它通过prometheus对应reload的http接口/-/reload，如果Operator监听到了配置的变化，就对调用此接口进行配置动态更新。具体细节可能会更加复杂一点。\nKube-Prometheus介绍 有了Kube-Prometheus之后，虽然知道它扩展很方便，但是我们还是希望有一个通用的配置，之后我们在通过修改配置去定制自己的监控集群。原因是一套完整的监控架构需要写的配置文件还是挺多的。:(\n于是就有了Kube-Prometheus项目，Kube-Prometheus项目可以通过一些配置快速的生成一套适应你集群的配置文件，之后你在修改生成的配置去适应你的集群，这样就节省了很多时间了。下面就介绍它是如何实现的。\n 使用官方推荐的Kube-Prometheus进行部署，原理是使用Jasonnet工具生成配置文件。 使用Helm社区维护的Chart部署  使用哪一种方法好？\n jsonnet本质是类似模版语言工具进行生成配置文件， helm 从某种程度上来说也是做这件事的，由于 Prometheus 相关社区(Grafana、Prometheus-Operator、 Kube-prometheus) 都使用 Jsonnet 做配置管理, 应该有它的理由。 Helm维护的配置文件没有Jsonnet来的完备、及时。（官方亲儿子） Json格式上的优势，它不需要考虑空格锁进问题。  我们这里就选择Jsonnet方式进行生成配置然后部署，在开始前先简单介绍jsonnet。\nJsonnet介绍 Jsonnet是一个模版语言工具，语法酷似Python，所以上手起来还是满快的。下面通过例子快速了解下它能做到什么，具体细节和语法推荐到官方上学习。jsonnet官网\n下面你可以跟着做，然后观察它的输出。当然，安装jsonnet是必须的，如果你还要使用Kube-Prometheus的话。\n 最后的引用库文件代码会生成一个库文件，除此之外不会有其他输出文件。\n   jsonnet 练习  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118  # 安装 Jsonnet(C 实现) brew install jsonnet # 也可以安装 Go 实现 go get github.com/google/go-jsonnet/cmd/jsonnet # 基本用法: 解释运行一个 jsonnet 源码文件 echo \u0026#34;{hello: \u0026#39;world\u0026#39;}\u0026#34; \u0026gt; test.jsonnet jsonnet test.jsonnet # 对于简单的代码也可以使用 -e 直接运行 jsonnet -e \u0026#39;{hello: \u0026#34;world\u0026#34;}\u0026#39; # 字段可以不加引号 自动格式化 jsonnet -e \u0026#39;{key: 1+2, hello: \u0026#34;world\u0026#34;}\u0026#39; # 类比: Jsonnet 支持与主流语言类似的四则运算, 条件语句, 字符串拼接,  # 字符串格式化, 数组拼接, 数组切片以及 python 风格的列表生成式 jsonnet - \u0026lt;\u0026lt;EOF { array: [1, 2] + [3], math: (4 + 5) / 3 * 2, format: \u0026#39;Hello, %s\u0026#39; % \u0026#39;world\u0026#39;, concat: \u0026#39;Hello, \u0026#39; + \u0026#39;world\u0026#39;, slice: [1,2,3,4][1:3], \u0026#39;list\u0026#39;: [x * x for x in [1,2,3,4]], condition: if 2 \u0026gt; 1 then \u0026#39;true\u0026#39; else \u0026#39;false\u0026#39;, } EOF # 使用变量: # 使用 :: 定义的字段是隐藏的(不会被输出到最后的 JSON 结果中),  # 这些字段可以作为内部变量使用(非常常用) # 使用 local 关键字也可以定义变量 # JSON 的值中可以引用字段或变量, 引用方式: # 变量名 # self 关键字: 指向当前对象 # $ 关键字: 指向根对象 jsonnet - \u0026lt;\u0026lt;EOF { local hello = \u0026#39;hello\u0026#39;, world:: \u0026#39;world\u0026#39;, message: { hello: hello, world: $.world, _hello: self.world, } } EOF # 使用函数: # 定义与引用方式与变量相同, 函数语法类似 python jsonnet - \u0026lt;\u0026lt;EOF { local hello(name) = \u0026#39;hello %s\u0026#39; % name, sum(x, y):: x + y, newObj(name=\u0026#39;hello\u0026#39;, age=23):: { name: name, age: age }, call_sum: $.sum(1, 2), call_hello: hello(\u0026#39;world\u0026#39;), me: $.newObj(age=24), } EOF # Jsonnet 使用组合来实现面向对象的特性(类似 Go) # Json Object 就是 Jsonnet 中的对象 # 使用 + 运算符来组合两个对象, 假如有字段冲突,  # 使用右侧对象(子对象)中的字段 # 子对象中使用 super 关键字可以引用父对象,  # 用这个办法可以访问父对象中被覆盖掉的字段 jsonnet - \u0026lt;\u0026lt;EOF local base = { o: \u0026#39;hello world\u0026#39;, no: { hello: \u0026#39;world\u0026#39; } }; base + { o: \u0026#39;hello zhangsan\u0026#39;, # +: 组合而非覆盖 no+: { nihao: \u0026#39;zhangsan\u0026#39; }, super_o: super.o } EOF # 库与 import: # jsonnet 共享库复用方式其实就是将库里的代码整合到当前文件中来, # 引用方式也很暴力, 使用 -J 参数指定 lib 文件夹, 再在代码里 import 即可 # jsonnet 约定库文件的后缀名为 .libsonnet cat \u0026gt; hello.libsonnet \u0026lt;\u0026lt;EOF { helloObj(hello=\u0026#39;world\u0026#39;, name):: { hello: hello, names: [], #声明数组 sayhello(name):: self + { names +: [name], }, } } EOF #import调用 jsonnet -J . - \u0026lt;\u0026lt;EOF local helloPackage = import \u0026#39;./hello.libsonnet\u0026#39;; helloPackage.helloObj(name=\u0026#39;world\u0026#39;) .sayhello(\u0026#39;zhangsan\u0026#39;) .sayhello(\u0026#39;lisi\u0026#39;) EOF      Kube-Prometheus安装、配置 有了上面的知识之后，就可以着手定制自己的配置文件了。我会逐一介绍，按照官方的配置的定制流程。\n首先看生成脚本\n  build.sh  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #!/usr/bin/env bash  # This script uses arg $1 (name of *.jsonnet file to use) to generate the manifests/*.yaml files. set -e set -x # only exit with zero if all commands of the pipeline exit successfully set -o pipefail # Make sure to use project tooling #PATH=\u0026#34;$(pwd)/tmp/bin:${PATH}\u0026#34; # Make sure to start with a clean \u0026#39;manifests\u0026#39; dir rm -rf manifests mkdir -p manifests/setup # 执行jsonnet生成定制的文件，引入的包为vendor，配置文件为脚本传入（默认为example.jsonnet）之后gojsontoyaml将json转化为yaml格式，最后输出到文件里 jsonnet -J vendor -m manifests \u0026#34;${1-example.jsonnet}\u0026#34; | xargs -I{} sh -c \u0026#39;cat {} | gojsontoyaml \u0026gt; {}.yaml\u0026#39; -- {} # Make sure to remove json files find manifests -type f ! -name \u0026#39;*.yaml\u0026#39; -delete rm -f kustomization     \n了解到安装方式之后，只要下载项目中的 vendor 包就可以了。之后创建一个定制的配置文件。安装完成之后的目录是这样的：\n1 2 3 4 5 6 7  . ├── build.sh #生成配置文件的脚本 ├── jsonnetfile.json #版本信息 ├── jsonnetfile.lock.json ├── monitor.jsonnet # 手动创建脚本的配置信息 └── vendor ├──...jsonnet依赖的包   之后编写定制文件：\n  monitor.jsonnet  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  local kp = (import \u0026#39;kube-prometheus/kube-prometheus.libsonnet\u0026#39;) + //基本配置 // (import \u0026#39;kube-prometheus/kube-prometheus-kube-aws.libsonnet\u0026#39;) + //Aws集群特殊配置 // (import \u0026#39;kube-prometheus/kube-prometheus-strip-limits.libsonnet\u0026#39;) + // Uncomment the following imports to enable its patches // (import \u0026#39;kube-prometheus/kube-prometheus-anti-affinity.libsonnet\u0026#39;) + // (import \u0026#39;kube-prometheus/kube-prometheus-managed-cluster.libsonnet\u0026#39;) + // (import \u0026#39;kube-prometheus/kube-prometheus-node-ports.libsonnet\u0026#39;) + // (import \u0026#39;kube-prometheus/kube-prometheus-static-etcd.libsonnet\u0026#39;) + // (import \u0026#39;kube-prometheus/kube-prometheus-thanos-sidecar.libsonnet\u0026#39;) + // (import \u0026#39;kube-prometheus/kube-prometheus-custom-metrics.libsonnet\u0026#39;) + // (import \u0026#39;kube-prometheus/kube-prometheus-external-metrics.libsonnet\u0026#39;) //+ // 配置全局或提供的动态配置，这里只设置了全局的命名空间 { _config+:: { namespace: \u0026#39;monitoring\u0026#39; } }; { [\u0026#39;setup/0namespace-\u0026#39; + name]: kp.kubePrometheus[name] for name in std.objectFields(kp.kubePrometheus) } + { [\u0026#39;setup/prometheus-operator-\u0026#39; + name]: kp.prometheusOperator[name] for name in std.filter((function(name) name != \u0026#39;serviceMonitor\u0026#39;), std.objectFields(kp.prometheusOperator)) } + // 如果要添加更多的export 在这里添加 // serviceMonitor is separated so that it can be created after the CRDs are ready { \u0026#39;prometheus-operator-serviceMonitor\u0026#39;: kp.prometheusOperator.serviceMonitor } + { [\u0026#39;node-exporter-\u0026#39; + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } + //{ [\u0026#39;blackbox-exporter-\u0026#39; + name]: kp.blackboxExporter[name] for name in std.objectFields(kp.blackboxExporter) } + #如果需要黑盒则放开 { [\u0026#39;kube-state-metrics-\u0026#39; + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } + { [\u0026#39;alertmanager-\u0026#39; + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } + { [\u0026#39;prometheus-\u0026#39; + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } + { [\u0026#39;prometheus-adapter-\u0026#39; + name]: kp.prometheusAdapter[name] for name in std.objectFields(kp.prometheusAdapter) } + { [\u0026#39;grafana-\u0026#39; + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) }     \n AWS集群由于kube-system中组件的label可能丢失，导致监控丢失。所以会帮我们自动生成service文件。这里只要记住如果是aws管理的集群添加这个是保险的。\n 运行脚本\n./build.sh monitor.jsonnet 脚本会生成监控相关的所有配置文件，CRD文件存放在./manifests/setup中，资源文件存放在./manifests中。查看manifests的文件如下。现在就可以开始修改一些配置信息，来满足你的需要了。接下来挑几个重要的文件，并添加常用的一些配置提供参考。\n  manifests/* and manifests/setup/*  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78  manifests # alertmanager相关配置 ├── alertmanager-alertmanager.yaml ├── alertmanager-secret.yaml # 告警配置 ├── alertmanager-service.yaml ├── alertmanager-serviceAccount.yaml ├── alertmanager-serviceMonitor.yaml # grafana相关配置 ├── grafana-dashboardDatasources.yaml ├── grafana-dashboardDefinitions.yaml ├── grafana-dashboardSources.yaml ├── grafana-deployment.yaml ├── grafana-service.yaml ├── grafana-serviceAccount.yaml ├── grafana-serviceMonitor.yaml # kubernetes集群指标监控 ├── kube-state-metrics-clusterRole.yaml ├── kube-state-metrics-clusterRoleBinding.yaml ├── kube-state-metrics-deployment.yaml ├── kube-state-metrics-service.yaml ├── kube-state-metrics-serviceAccount.yaml ├── kube-state-metrics-serviceMonitor.yaml # 各个几点指标监控 ├── node-exporter-clusterRole.yaml ├── node-exporter-clusterRoleBinding.yaml ├── node-exporter-daemonset.yaml ├── node-exporter-service.yaml ├── node-exporter-serviceAccount.yaml ├── node-exporter-serviceMonitor.yaml # kubernetes hpa相关适配器 ├── prometheus-adapter-apiService.yaml ├── prometheus-adapter-clusterRole.yaml ├── prometheus-adapter-clusterRoleAggregatedMetricsReader.yaml ├── prometheus-adapter-clusterRoleBinding.yaml ├── prometheus-adapter-clusterRoleBindingDelegator.yaml ├── prometheus-adapter-clusterRoleServerResources.yaml ├── prometheus-adapter-configMap.yaml ├── prometheus-adapter-deployment.yaml ├── prometheus-adapter-roleBindingAuthReader.yaml ├── prometheus-adapter-service.yaml ├── prometheus-adapter-serviceAccount.yaml ├── prometheus-adapter-serviceMonitor.yaml # prometheus相关配置 ├── prometheus-clusterRole.yaml ├── prometheus-clusterRoleBinding.yaml ├── prometheus-kubeControllerManagerPrometheusDiscoveryService.yaml ├── prometheus-kubeSchedulerPrometheusDiscoveryService.yaml ├── prometheus-operator-serviceMonitor.yaml ├── prometheus-prometheus.yaml # 监控配置 ├── prometheus-roleBindingConfig.yaml ├── prometheus-roleBindingSpecificNamespaces.yaml ├── prometheus-roleConfig.yaml ├── prometheus-roleSpecificNamespaces.yaml ├── prometheus-rules.yaml # 默认监控项 ├── prometheus-service.yaml ├── prometheus-serviceAccount.yaml # serviceMonitor相关配置 ├── prometheus-serviceMonitor.yaml ├── prometheus-serviceMonitorApiserver.yaml ├── prometheus-serviceMonitorCoreDNS.yaml ├── prometheus-serviceMonitorKubeControllerManager.yaml ├── prometheus-serviceMonitorKubeScheduler.yaml ├── prometheus-serviceMonitorKubelet.yaml └── setup # CRD文件 ├── 0namespace-namespace.yaml ├── prometheus-operator-0alertmanagerConfigCustomResourceDefinition.yaml ├── prometheus-operator-0alertmanagerCustomResourceDefinition.yaml ├── prometheus-operator-0podmonitorCustomResourceDefinition.yaml ├── prometheus-operator-0probeCustomResourceDefinition.yaml ├── prometheus-operator-0prometheusCustomResourceDefinition.yaml ├── prometheus-operator-0prometheusruleCustomResourceDefinition.yaml ├── prometheus-operator-0servicemonitorCustomResourceDefinition.yaml ├── prometheus-operator-0thanosrulerCustomResourceDefinition.yaml ├── prometheus-operator-clusterRole.yaml ├── prometheus-operator-clusterRoleBinding.yaml ├── prometheus-operator-deployment.yaml ├── prometheus-operator-service.yaml └── prometheus-operator-serviceAccount.yaml      prometheus配置 查看 prometheus-prometheus.yaml 文件\n  prometheus-prometheus.yaml  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  apiVersion:monitoring.coreos.com/v1kind:Prometheusmetadata:labels:prometheus:k8sname:k8snamespace:monitoringspec:alerting:alertmanagers:- name:alertmanager-mainnamespace:monitoringport:webimage:quay.io/prometheus/prometheus:v2.22.1nodeSelector:kubernetes.io/os:linuxpodMonitorNamespaceSelector:{}podMonitorSelector:{}probeNamespaceSelector:{}probeSelector:{}replicas:2# 设置副本数retention:15d# 添加数据保留期限# 如果需要添加集群外部数据采集， 在这里添加配置文件#additionalScrapeConfigs:#name: additional-scrape-configs#key: prometheus-additional.yaml#添加scstorage:volumeClaimTemplate:spec:storageClassName:xxxresources:requests:storage:50Giresources:requests:memory:400MiruleSelector:matchLabels:prometheus:k8srole:alert-rulessecurityContext:fsGroup:2000runAsNonRoot:truerunAsUser:1000serviceAccountName:prometheus-k8sserviceMonitorNamespaceSelector:{}serviceMonitorSelector:{}version:v2.22.1    \n如果配置了外部采集节点，创建 prometheus-additional-secret.yaml：\n  prometheus-additional-secret.yaml  1 2 3 4 5 6 7 8 9 10 11 12 13 14  # prometheus-additional-secret.yamlapiVersion:v1data:{}kind:Secretmetadata:name:additional-scrape-configsnamespace:monitoringstringData:prometheus-additional.yaml:|-- job_name: \u0026#34;prometheus-demo\u0026#34; metrics_path: \u0026#34;metrics\u0026#34; static_configs: - targets: [\u0026#34;xxx:8080\u0026#34;]type:Opaque    \nprometheus-rule配置 查看 prometheus-rules.yaml 文件\n可以看到该项目内置了很多告警项，可以根据自己需要删减。过多的rule可能会影响性能和消耗更多的内存，所以还是根据自己的需要来配置。当然这些rule对于后面Grafana展示都是有用到的。Grafana中的图表也是根据这些规则配置好了的，两者是相关联的。\nalertmanager-secret.yaml配置 创建 alertmanager-secret.yaml文件。具体需要的配置看 alertmanager config。\n  alertmanager-secret.yaml  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  apiVersion:v1data:{}kind:Secretmetadata:name:alertmanager-mainnamespace:monitoringstringData:alertmanager.yaml:|-global: resolve_timeout: 2h smtp_smarthost: \u0026#39;smtp.qq.com:465\u0026#39; smtp_from: \u0026#39;xxx@xx.com\u0026#39; smtp_auth_username: \u0026#39;xxx@xx.com\u0026#39; smtp_auth_password: \u0026#39;xxx\u0026#39; smtp_require_tls: false route: group_by: [\u0026#39;alertname\u0026#39;] group_wait: 30s group_interval: 5m repeat_interval: 4h receiver: default routes: - receiver: \u0026#39;email-receiver\u0026#39; group_wait: 30s match: notify: notify receivers: - name: email-receiver email_configs: - to: \u0026#39;xxx@xx.com\u0026#39; send_resolved: true - to: \u0026#39;xxx@xx.com\u0026#39; send_resolved: true - name: \u0026#39;default\u0026#39; webhook_configs: - url: \u0026#39;http://xxx\u0026#39; send_resolved: truetype:Opaque    \n查看监控信息 完成配置定制之后运行\n1 2  kubectl apply -f manifests/setup kubectl apply -f manifests/   以下内容为将端口映射到本地主机端口\nPrometheus Grafana Alertmanager  kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 访问地址 http://localhost:9090\n kubectl --namespace monitoring port-forward svc/grafana 3000 访问地址 http://localhost:3000 用户名和密码为 admin:admin\n kubectl --namespace monitoring port-forward svc/alertmanager-main 9093 访问地址 http://localhost:9093\n   'use strict'; var containerId = JSON.parse(\"\\\"1aad72f59b0458b7\\\"\"); var containerElem = document.getElementById(containerId); var codetabLinks = null; var codetabContents = null; var ids = []; if (containerElem) { codetabLinks = containerElem.querySelectorAll('.codetab__link'); codetabContents = containerElem.querySelectorAll('.codetab__content'); } for (var i = 0; i 0) { codetabContents[0].style.display = 'block'; } \n扩展 Grafana仪表盘图 参考 https://grafana.com/grafana/dashboards\n更多监控报警规则参考 https://awesome-prometheus-alerts.grep.to/rules\n遇到的问题 manifests/prometheus-rules.yaml 部署失败 Error from server (InternalError): error when creating \u0026ldquo;manifests/prometheus-rules.yaml\u0026rdquo;: Internal error occurred: failed calling webhook \u0026ldquo;prometheusrulemutate.monitoring.coreos.com\u0026rdquo;: Post https://kube-prometheus-stack-1607-operator.default.svc:443/admission-prometheusrules/mutate?timeout=10s: service \u0026ldquo;kube-prometheus-stack-1607-operator\u0026rdquo; not found 具体问题查看 https://github.com/helm/charts/issues/21080\n解决：\n1 2 3 4 5 6 7 8  kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io #do 删除 kube-prometheus..资源 kubectl get MutatingWebhookConfiguration #do 删除 kube-prometheus..资源 #重新载入 kubectl apply -f manifests/prometheus-rules.yaml   两个监控任务没有对应的目标 访问 Prometheus\n遇到这种情况一般是你下载的版本和你的Kubernetes集群版本不匹配，导致label丢失，所以ServiceMonitor找不到对应的Service。\n解决思路\n 查看 ServiceMonitor 对应的服务端口是否存在 服务端口是否匹配到正确到po（查看po是否含有匹配标签） 通信协议是否正确（http/https）  ","description":"本文介绍如何使用开源项目 Kube-Prometheus 监控 Kubernetes 集群，以及如何扩展。","id":1,"section":"posts","tags":["prometheus","kubernetes"],"title":"Prometheus 监控 Kubernetes 集群","uri":"/posts/prometheus%E7%9B%91%E6%8E%A7kubenetes%E9%9B%86%E7%BE%A4/"},{"content":"初始化代码库 1 2 3 4 5 6  # 在当前目录新建一个Git代码库 git init # 新建一个目录，将其初始化为Git代码库 git init [project-name] # 下载一个项目和它的整个代码历史 git clone [url]   配置全局信息 1 2 3 4 5 6 7  # 显示当前的Git配置 git config --list # 编辑Git配置文件 git config -e [--global] # 设置提交代码时的用户信息 git config [--global] user.name \u0026#34;[name]\u0026#34; git config [--global] user.email \u0026#34;[email address]\u0026#34;   代码提交 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 添加当前目录的所有文件到暂存区 git add . # 提交暂存区到仓库区 git commit -m [message] # 提交暂存区的指定文件到仓库区 git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 git commit -a # 提交时显示所有diff信息 git commit -v # 使用一次新的commit，替代上一次提交，如果代码没有任何新变化，则用来改写上一次commit的提交信息 git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 git commit --amend [file1] [file2] ...   分支操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  # 列出所有本地分支 git branch # 列出所有远程分支 git branch -r # 列出所有本地分支和远程分支 git branch -a # 新建一个分支，但依然停留在当前分支 git branch [branch-name] # 新建一个分支，并切换到该分支 git checkout -b [branch] # 新建一个分支，指向指定commit git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 git checkout [branch-name] # 切换到上一个分支 git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 git merge [branch] # 选择一个commit，合并进当前分支 git cherry-pick [commit] # 删除分支 git branch -d [branch-name] # 删除远程分支 git push origin --delete [branch-name] git branch -dr [remote/branch]   远端仓库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # 下载远程仓库的所有变动 git fetch [remote] # 显示所有远程仓库 git remote -v # 显示某个远程仓库的信息 git remote show [remote] # 增加一个新的远程仓库，并命名 git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 git pull [remote] [branch] # 上传本地指定分支到远程仓库 git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 git push [remote] --force # 推送所有分支到远程仓库 git push [remote] --all   操作集合 master* : 本地分支（* 代表当前代码所在分支）\norigin/master : 你的git仓库里的分支\nupstream/master : Fork 项目的源地址  跟随Fork项目 1 2 3 4 5 6 7 8 9 10 11 12 13 14  #添加 upstream/master 分支地址，如果没有到话 git remote add upstream [url] #拉取 upstream/master 分支代码 git fetch upstream #切换到 upstream/master 分支 git checkout upstream/master # 创建一个新分支（这支分支只负责同步 upstream/master 的代码） git branch -b tmp #把新建分支和 upstream/master 分支关联 git branch --set-upstream-to=upstream/master   做完以上操作之后，本地就有一份 upstream/master 分支的代码了。现在就可以通过合并分支的一些操作去同步代码了。由于我们还关联了 upstream/master 的分支地址，所以每次只要切换到 tmp 分支进行 pull 即可获取到最新代码。\nsubmodule 在初始化含有submodule项目时，需要手动更新submodule代码。\n1 2 3 4  git submodule init \u0026amp;\u0026amp; git submodule update #如果直接 clone 目标项目时可以直接带上 --recursive git clone https://xxx.git --recursive   如果一个项目中包含多个submodule，则需要进入每个文件夹中进行代码拉取。但是git提供了一种更简便的方法。\n1 2  # git submodule \u0026lt;command\u0026gt; 可以遍历所有的submodule，并执行相应的命令 git submodule foreach git checkout master   参考 常用 Git 命令清单\n关于 git-submodule 的一些基本操作\n","description":"git 常用命令和操作","id":2,"section":"posts","tags":["git"],"title":"git 操作命令","uri":"/posts/git%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"},{"content":"CentOS  运行环境 安装 1 2  # 安装go yum -y install golang   添加epel repo vim /etc/yum.repos.d/hugo.repo\n1 2 3 4 5 6 7 8 9  [daftaupe-hugo] name=Copr repo for hugo owned by daftaupe baseurl=https://copr-be.cloud.fedoraproject.org/results/daftaupe/hugo/epel-7-$basearch/ type=rpm-md skip_if_unavailable=True gpgcheck=1 gpgkey=https://copr-be.cloud.fedoraproject.org/results/daftaupe/hugo/pubkey.gpg repo_gpgcheck=0 enabled=1   1 2 3 4  # init hugo serve # 发布新文章 # hugo serve -e production --disableFastRender # 生产环境, enables full re-renders on changes   常用参数介绍 1 2 3 4 5 6 7 8 9 10 11  --bind=\u0026#34;127.0.0.1\u0026#34; #服务监听IP地址； -p, --port=1313 #服务监听端口； -w, --watch[=true] #监听站点目录，发现文件变更自动编译； -D, --buildDrafts #包括被标记为draft的文章； -E, --buildExpired #包括已过期的文章； -F, --buildFuture #包括将在未来发布的文章； -b, --baseURL=\u0026#34;www.datals.com\u0026#34; #服务监听域名； --log[=false]: #开启日志； --logFile=\u0026#34;/var/log/hugo.log\u0026#34;: #log输出路径； -t, --theme=\u0026#34;\u0026#34; #指定主题； -v, --verbose[=false]: #输出详细信息   常用使用参数组合 1  hugo server -t hyde --buildDrafts --baseURL=http://www.xxx.com --bind=0.0.0.0 --port=80 -w   发布 执行hugo命令，站点目录下会新建文件夹public/，生成的所有静态网站页面都会存储到这个目录。\n 如果使用Github pages来作为博客的Host，你只需要将public/里的文件上传就可以。 如果使用nginx作为web服务配置root dir 指向public/ 即可。  参考 Hugo 常用命令详解\n","description":"Hugo博客搭建记录。","id":3,"section":"posts","tags":["hugo"],"title":"Blog搭建笔记","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/"},{"content":"你好哇，本博客记录自己认为需要记录的东西，原因是这些内容没有不记录的理由。由此也可知，它们本无教学目的，仅供参考。如需要转载，请附上地址，不胜感激。\n在本站中所有文字，有一部分参考自别家博客，但凡有涉及到的都在文末附上参考连接。但由于历史遗留问题，如果发现遗漏，请联系我，定当立马处理。  ","description":"","id":4,"section":"","tags":null,"title":"关于我","uri":"/about/"},{"content":"文章图片引用图来自 Pixabay\n","description":"Photo of this blog","id":5,"section":"gallery","tags":null,"title":"Photo","uri":"/gallery/photo/"},{"content":"#\u0026mdash;\n#title: \u0026ldquo;Hugo zDoc Theme\u0026rdquo;\n#date: 2020-01-19T21:13:42+09:00\n#description: Make a documentation with hugo zdoc theme!\n#weight: 1\n#link: https://github.com/zzossig/hugo-theme-zdoc\n#repo: https://github.com/zzossig/hugo-theme-zdoc\n#pinned: true\n#thumb: feature3/css3-bare.png\n#\u0026mdash;\n","description":"","id":6,"section":"showcase","tags":null,"title":"","uri":"/showcase/hugo/hugo-theme-zdoc/"},{"content":"#\u0026mdash;\n#title: \u0026ldquo;Hugo Zzo Theme\u0026rdquo;\n#date: 2020-01-19T21:13:42+09:00\n#description: Make a blog with hugo zzo theme!\n#weight: 2\n#link: https://github.com/zzossig/hugo-theme-zzo\n#repo: https://github.com/zzossig/hugo-theme-zzo\n#pinned: true\n#thumb: feature3/css3.png\n#\u0026mdash;\n","description":"","id":7,"section":"showcase","tags":null,"title":"","uri":"/showcase/hugo/hugo-theme-zzo/"}]